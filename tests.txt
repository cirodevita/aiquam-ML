# KNN

python main.py --is_training 1 --model KNN --data UEA --root_path ./dataset/AIQUAM/

# CNN

python main.py --is_training 1 --model CNN --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --dropout 0.5 --learning_rate=0.0005 --patience 10 --gamma 1.5 --alpha 0.35
python main.py --is_training 0 --model CNN --data UEA --root_path ./dataset/AIQUAM/ --convert 1

# DLINEAR

python main.py --is_training 1 --model DLinear --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --learning_rate 0.005 --gamma 2.0 --alpha 0.75
python main.py --is_training 0 --model DLinear --data UEA --root_path ./dataset/AIQUAM/ --convert 1

# INFORMER

python main.py --is_training 1 --model Informer --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 32 --patience 10 --learning_rate 0.001 --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --gamma 1.5 --alpha 0.5
python main.py --is_training 0 --model Informer --data UEA --root_path ./dataset/AIQUAM/ --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --convert 1

# REFORMER

python main.py --is_training 1 --model Reformer --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --patience 10 --learning_rate 0.001 --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --gamma 1.5 --alpha 0.35
python main.py --is_training 0 --model Reformer --data UEA --root_path ./dataset/AIQUAM/ --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --convert 1

# TRANSFORMER

python main.py --is_training 1 --model Transformer --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --patience 10 --learning_rate 0.0005 --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --gamma 1.5 --alpha 0.35
python main.py --is_training 0 --model Transformer --data UEA --root_path ./dataset/AIQUAM/ --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --convert 1

# TIMESNET

python main.py --is_training 1 --model TimesNet --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --patience 10 --learning_rate 0.001 --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --gamma 1.5 --alpha 0.35
python main.py --is_training 0 --model TimesNet --data UEA --root_path ./dataset/AIQUAM/ --e_layers 3 --top_k 3 --d_model 128 --d_ff 128 --convert 1



# NONSTATIONARY_TRANSFORMER

python main.py --is_training 1 --model Nonstationary_Transformer --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --patience 10 --learning_rate 0.001 --e_layers 3 --top_k 3 --d_model 128
python main.py --is_training 0 --model Nonstationary_Transformer --data UEA --root_path ./dataset/AIQUAM/ --itr 1 --train_epochs 100 --batch_size 16 --patience 10 --learning_rate 0.001 --e_layers 3 --top_k 3 --d_model 128 --convert 1
